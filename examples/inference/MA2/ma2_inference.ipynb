{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood-Free Parameter Inference on the MA2 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates neural network-based (henceforth referred to as ANN) inference and approximate Bayesian computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ANN models learn the relationship ${\\bf y} \\rightarrow {\\bf \\theta}$, where ${\\bf y}$ is a time series response and ${\\bf \\theta}$ are the parameters of the descriptive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we require a training set of the form $({\\bf y, \\theta})$ to train the ANN models. We will first generate such a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ma2_model import simulate, prior\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simulation\n",
    "sim = simulate\n",
    "true_param = [0.6, 0.2]  # true \\theta moving average2\n",
    "\n",
    "data = simulate(true_param)\n",
    "\n",
    "n = 10000\n",
    "train_thetas = np.array(prior(n=n))\n",
    "train_ts = np.expand_dims(np.array([simulate(p, n=100) for p in train_thetas]), 2)\n",
    "\n",
    "validation_thetas = np.array(prior(n=10000))\n",
    "validation_ts = np.expand_dims(np.array([simulate(p, n=100) for p in validation_thetas]), 2)\n",
    "\n",
    "test_thetas = np.array(prior(n=10000))\n",
    "test_ts = np.expand_dims(np.array([simulate(p, n=100) for p in validation_thetas]), 2)\n",
    "\n",
    "abc_trial_thetas = np.array(prior(n=30000))\n",
    "abc_trial_ts = np.expand_dims(np.array([simulate(p, n=100) for p in abc_trial_thetas]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training data in the right format\n",
    "train_ts = train_ts.transpose((0, 2, 1))\n",
    "validation_ts = validation_ts.transpose((0, 2, 1))\n",
    "test_ts = test_ts.transpose((0, 2, 1))\n",
    "data = data.reshape(1,1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set input and output shape for the CNN\n",
    "input_shape = (100,1)\n",
    "output_shape = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the search space for inference\n",
    "dmin = [-2, -1]\n",
    "dmax = [4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routines to normalize and denormalize data\n",
    "# Makes training easier\n",
    "def normalize_data(data, dmin, dmax):\n",
    "    dmin = np.array(dmin)\n",
    "    dmax = np.array(dmax)\n",
    "    return (data - dmin)/(dmax-dmin)\n",
    "\n",
    "def denormalize_data(data, dmin, dmax):\n",
    "    dmin = np.array(dmin)\n",
    "    dmax = np.array(dmax)\n",
    "    denorm = data * (dmax-dmin) + dmin\n",
    "    return denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_thetas = normalize_data(train_thetas, dmin, dmax)\n",
    "normed_thetas_val = normalize_data(validation_thetas, dmin, dmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ANN models\n",
    "from sciope.models.cnn_regressor import CNNModel\n",
    "from sciope.models.dnn_regressor import DNNModel\n",
    "from sciope.models.pen_regressor import PENModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "model_cnn = CNNModel(input_shape, output_shape)\n",
    "model_dnn = DNNModel(input_shape, output_shape)\n",
    "model_pen = PENModel(input_shape, output_shape, pen_nr=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = model_cnn.train(train_ts, normed_thetas, batch_size=256, \n",
    "                      epochs=500, verbose=0, learning_rate=0.001, \n",
    "                      early_stopping_patience=5,\n",
    "                      validation_inputs=validation_ts, validation_targets=normed_thetas_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dnn = model_dnn.train(train_ts, normed_thetas, batch_size=256, \n",
    "                      epochs=500, verbose=0, learning_rate=0.001, \n",
    "                      early_stopping_patience=5,\n",
    "                      validation_inputs=validation_ts, validation_targets=normed_thetas_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_pen = model_pen.train(train_ts, normed_thetas, batch_size=256, \n",
    "                      epochs=500, verbose=0, learning_rate=0.001, \n",
    "                      early_stopping_patience=5,\n",
    "                      validation_inputs=validation_ts, validation_targets=normed_thetas_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Routine to test each ANN architecture on MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "def test_model(model):\n",
    "    pred_test = model.predict(test_ts)\n",
    "    pred_test = denormalize_data(pred_test, dmin, dmax)\n",
    "    #mae_test = np.mean(abs(pred_test - samples_test), axis=0)\n",
    "    mae_test = mean_absolute_error(test_thetas, pred_test)\n",
    "    \n",
    "    theta_pred = model.predict(data)\n",
    "    samples_true = np.asarray(true_param)\n",
    "    theta_pred = denormalize_data(theta_pred, dmin, dmax)\n",
    "    mae_true = np.mean(np.abs(theta_pred.ravel() - samples_true), axis=0)\n",
    "    #mae_true = mean_absolute_error(np.asarray(samples_true).reshape(1,2), np.asarray(theta_pred).reshape(1,2))\n",
    "    return mae_test, mae_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test metrics\n",
    "mae_test_cnn, mae_true_cnn = test_model(model_cnn)\n",
    "mae_test_dnn, mae_true_dnn = test_model(model_dnn)\n",
    "mae_test_pen, mae_true_pen = test_model(model_pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN test MAE = 0.7031869420803443, MAE at true point = 0.1392668604850769\n",
      "DNN test MAE = 0.7157589187285183, MAE at true point = 0.2387733519077301\n",
      "PEN test MAE = 0.7589048176817108, MAE at true point = 0.12148187458515167\n"
     ]
    }
   ],
   "source": [
    "print(\"CNN test MAE = {}, MAE at true point = {}\".format(mae_test_cnn, mae_true_cnn))\n",
    "print(\"DNN test MAE = {}, MAE at true point = {}\".format(mae_test_dnn, mae_true_dnn))\n",
    "print(\"PEN test MAE = {}, MAE at true point = {}\".format(mae_test_pen, mae_true_pen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we used the ANN models to directly infer the parameters from given time series. The ANN models can also be used as summary statistics in conjunction with ABC inference. The following Class implements one such summary statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciope.utilities.summarystats.summary_base import SummaryBase\n",
    "from sciope.utilities.housekeeping import sciope_logger as ml\n",
    "\n",
    "class ANN_Statistics(SummaryBase):\n",
    "    \"\"\"\n",
    "    The thetas predicted by ANN models act as summary statistics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean_trajectories=False, use_logger=False):\n",
    "        self.name = 'ANN_Statistics'\n",
    "        super(ANN_Statistics, self).__init__(self.name, mean_trajectories, use_logger)\n",
    "        if self.use_logger:\n",
    "            self.logger = ml.SciopeLogger().get_logger()\n",
    "            self.logger.info(\"ANN_Statistics summary statistic initialized\")\n",
    "\n",
    "    def compute(self, data):\n",
    "        \"\"\"\n",
    "        Calculate the value(s) of the summary statistic(s)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : [type]\n",
    "            simulated or data set in the form N x S X T - num data points x num species x num time steps\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        [type]\n",
    "            computed statistic value\n",
    "        \n",
    "        \"\"\"\n",
    "        data_arr = np.array(data)\n",
    "        assert len(data_arr.shape) == 3, \"required input shape is (n_points, n_species, n_timepoints)\"\n",
    "\n",
    "        res = model_cnn.predict(data_arr)\n",
    "        res = denormalize_data(res, dmin, dmax)\n",
    "\n",
    "        if self.mean_trajectories:\n",
    "            res = np.asarray(np.mean(res, axis=0))  # returns a scalar, so we cast it as an array\n",
    "\n",
    "        if self.use_logger:\n",
    "            self.logger.info(\"ANN_Statistics summary statistic: processed data matrix of shape {0} and generated summaries\"\n",
    "                             \" of shape {1}\".format(data.shape, res.shape))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our new summary statistic\n",
    "cnn_stat = ANN_Statistics()\n",
    "predicted_stat = cnn_stat.compute(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36439425 0.24292797]]\n",
      "MAE upon comparison as a statistic = 0.1392668604850769\n"
     ]
    }
   ],
   "source": [
    "print(predicted_stat)\n",
    "stat_mae = np.mean(np.abs(predicted_stat.ravel() - np.asarray(true_param)), axis=0)\n",
    "print(\"MAE upon comparison as a statistic = {}\".format(stat_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to set up ABC inference. Here we use Burstiness as a summary statistic. Auto-covariance is more appropriate for MA(2). We can use our ANN model as well, but it should be trained on far more points in order to be accurate enough for use in inference. This notebook is meant to be quick to execute, and so we have used a small training set for exposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MA2 simulator function\n",
    "def ma2_sim(param):\n",
    "    res = np.expand_dims(np.array([simulate(param, n=100)]), 2)\n",
    "    res = res.transpose((0, 2, 1))    # reshape to N x S X T\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate observed data\n",
    "obs_data = []\n",
    "for i in range(20):\n",
    "    obs_data.append(ma2_sim(true_param))\n",
    "obs_data = np.asarray(obs_data)\n",
    "obs_data = obs_data[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciope.inference import abc_inference\n",
    "from sciope.utilities.priors import uniform_prior\n",
    "from sciope.utilities.summarystats import burstiness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_function = uniform_prior.UniformPrior(dmin, dmax)\n",
    "bs_stat = burstiness.Burstiness()\n",
    "abc_instance = abc_inference.ABC(obs_data, ma2_sim, prior_function, epsilon=0.01, \n",
    "                                 #summaries_function=cnn_stat.compute, # ensure CNN is accurate enough before use\n",
    "                                 summaries_function=bs_stat.compute,\n",
    "                                 summaries_divisor=np.max(abc_trial_thetas, axis=0), use_logger=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_instance.compute_fixed_mean(chunk_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_results = abc_instance.infer(num_samples=50, batch_size=10, chunk_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC mean inference error = 1.1916615337995151\n"
     ]
    }
   ],
   "source": [
    "# Calculate MAE\n",
    "acc_samples = np.asarray(abc_results['accepted_samples'])\n",
    "abc_mae = np.mean(np.abs(acc_samples - np.asarray(true_param).reshape(1, 2)), axis=0)\n",
    "abc_mae = np.mean(abc_mae)   # mean of both thetas\n",
    "print(\"ABC mean inference error = {}\".format(abc_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
